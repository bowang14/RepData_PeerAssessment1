---
title: "Prediction"
author: "Wang Bo"
date: "Oct 25, 2015"
output: html_document
---
In this document, I will fit a model based on the training set. This is a dataset recording the movements of 6 subjects. I will use these parameters to predict which types of movement are they doing.

1. Loading the Data and Packages Needed
```{r}
training=read.csv('pml-training.csv')
testing=read.csv('pml-testing.csv')
library(caret)
```

2. NA-columns Removal
Since there are some NA values in the dataset, I need to remove them. I search in testing set because I can only use data availuable to predict.
```{r}
t=0
for(i in 1:ncol(testing)){
  t[i]=sum(is.na(testing[,i]))
}
t=as.logical(!t)
training1=training[,t]
testing1=testing[,t]
```

3. Near Zero Value Removal
There are some near zero values (nzv) in the set and I have to remove them.
```{r}
nsv =nearZeroVar(training1,saveMetrics=TRUE)
training1=training1[,!nsv$nzv]
testing1=testing1[,!nsv$nzv]
```

4. Data Type Change
Only when all variables are in numeric or integer type can I use train function to fit a model. So I need to change all factor variables into numeric. 

There are classmates argue in forum that classe variable should be in factor type, but it's not true on my computer. When classe ia in factor type, there will be an error message after train function.
```{r}
for(i in 1:ncol(training1)){
  if(class(training1[,i])=="factor"){
    training1[,i]=seq_along(levels(training1[,i]))[training1[,i]]
    testing1[,i]=seq_along(levels(testing1[,i]))[testing1[,i]]
  }
}
```

5. Split the Data
In this part, I split the training1 set into train and test subsets named 'trainset' and 'testset' respectively. Considering reproduciblity, I first set a seed to 25.
```{r}
set.seed(25)
inTrain=createDataPartition(y=training1$classe,p=0.7,list=F)
trainset=training1[inTrain,]
testset=training1[-inTrain,]
```

6. Folds Creating
In order to show cross validation, I need to subset the train set. I adopt the random subsampling method.
```{r}
folds=createResample(y=trainset$classe,times=10,list=T)
sapply(folds,length)
```

We can see from the results that there are 10 folds and how many items are in each fold.

7. Method Selecting
In this part, I start to fit a model. However, I fail to use the random forest method on my computer. When I use rf method, there will be an error message.
```{r,eval=FALSE}
fit=train(classe~.,data=trainset,method='rf',prox=T)

##Something is wrong; all the RMSE metric values are missing:
##  RMSE        Rsquared  
##Min.   : NA   Min.   : NA  
##1st Qu.: NA   1st Qu.: NA  
##Median : NA   Median : NA  
##Mean   :NaN   Mean   :NaN  
##3rd Qu.: NA   3rd Qu.: NA  
##Max.   : NA   Max.   : NA  
##NA's   :3     NA's   :3    
##Error in train.default(x, y, weights = w, ...) : Stopping
##In addition: There were 50 or more warnings (use warnings() to see the first 50)
```

However, it works well if I run it on a smaller subset.
```{r,eval=FALSE}
trainset1=trainset[1:1000,]
fit=train(classe~.,data=trainset1,method='rf',prox=T)
```

So I think this is because the dataset is too large and my computer is too old. I have to give up rf method and adopt generalized linear model (glm) method.

8. Model Fitting
For each of 10 folds, I treat it as test set and others as train set to fit a model. Then I select the model with the highest accurary as the final model. More details can be found from the explaination in the following code.
```{r,cache=T}
fit=vector()
accuracystar=0
for(i in 1:10){
  ##for each loop, choose the train and test index
  trainfolds=folds[-i]
  testfolds=folds[[i]]
  
  ##remove repeated values
  indextrain=vector()
  for(j in 1:9){indextrain=c(indextrain,trainfolds[[j]])}
  indextrain=unique(indextrain,fromLast=T)
  trainsub=trainset[indextrain,]
  
  indextest=unique(testfolds,fromLast=T)
  testsub=trainset[indextest,]
  
  ##fit the model
  fit=train(classe~.,data=trainsub,method='glm')
  
  ##make the prediction
  prediction=round(predict(fit,newdata=testsub))
  confusion=confusionMatrix(prediction,testsub$classe)
 
  ##make sure if this model is better then befofe, if so, replace the optimal variables
  if(accuracystar<confusion$overall[1]){
    accuracystar=confusion$overall[1]
    fitstar=fit
  }
}

##make the final prediction with the model on test set.
prediction=round(predict(fitstar,newdata=testset))
confusion=confusionMatrix(prediction,testset$classe)
fitstar
fitstar$finalModel
confusion
```

9. Modeling Result
From this result, we can see that the model fits well both in sub-testsub and in testset. The accurarcy is 1 means the out of sample error is almost the same with the in sample error. So there must be some overfitting.

When I use this model to predict testing1, problems occurs.
```{r}
prediction=round(predict(fitstar,newdata=testing1))
prediction
```

There are only 0 and 1. However, level A to E are corresponding with 1 to 5.

I think this is maybe because the glm method is not so good in this case.